# Test of FVN algorithm

## Motivation of FVN
The idea behind FVN is similar to the idea of kNN--to predict the behavior of someone by looking at what his or her neighbors do, and assume his or her behavior is the average of these neighbors. Instead of looking at a fixed number of nearest neighbors like what kNN does, FVN looks at all the neighbors within a fixed distance from the person we want to predict. While kNN always yields a prediction in the variable space no matter where, FVN does not give a prediction if that someone we try to predict has no neighbor within the distance that we specify. This is usually a disadvantage but sometimes can be viewed as an advantage if we don't really trust a prediction based on far-away neighbors.

## User parameters of FVN
As said in the README file, the distance in the variable space that we care about is the normalized distance. It means that the distribution of the reference data points in each dimension is normalized so that its standard deviation equals 1, then the distance is calculated.
The first parameter needed is the size of the neighborhood within which the data points are official neighbors. Instead of specifying directly the normalized radius of the hyper-dimensional neighborhood ball, we specify the "normalized volume". In other words, the user defines the volume of the neighborhood in comparison to the volume of the whole reference data distribution in the variable space.
The 2nd parameter is the minimum number of data points in the neighborhood for a reasonable prediction.
The 3rd parameter is used in categorical prediction only. It's the minimum fraction of the most popular class in the neighborhood where we can take this class as the predicted response.

## What's the test data
In the following I show a case using the FVN algorithm. First some libraries and the Cars93 data are imported. The FVN function is created in "FVN.R". The Cars93 dataset is saved in "data".
```{r}
rm(list = ls())
suppressMessages(library(dplyr))
suppressMessages(library(data.table))
suppressMessages(require(MASS))
suppressMessages(source("fvn.R"))
data <- Cars93
```
There are 93 objects and 27 variables. Each object is a car, and each variable is a measure of the car, which contains horsepower, size, number of passengers, MPG, price, etc. I will like to take a few numerical variables as predictors and the price of the car as the response. Before defining the exact predictors, I first separate the dataset into the reference dataset and the query dataset. We use the reference data points to predict the response of the query data points. Here I random sample 2/3 objects as reference data points and the rest as query data points.
```{r}
data$samp <- sample(nrow(data), nrow(data))
nref <- as.integer(nrow(data) * 0.66)
ref <-
  data %>%
  dplyr::filter(samp <= nref) %>%
  dplyr::select(-samp)
que <-
  data %>%
  dplyr::filter(samp > nref) %>%
  dplyr::select(-samp)
```

## Run FVN with 2 to 4 predictor variables
There are 3 examples in the following, with 2, 3, and 4 predictor variables. Each time when there's a new predictor (which I think is the next important representitive feature of a car), the prediction changes somehow.

### Example 1: use horsepower and number of passengers to predict the price
```{r}
dref <- ref %>% dplyr::select(Horsepower, Passengers, Price)
dque <- que %>% dplyr::select(Horsepower, Passengers, Price)
m <- ncol(dref)
p <-
  fvn(
    x = dref[, -m],
    y = dref[, m],
    q = dque[, -m],
    v = 0.2,
    min.pts = 1,
    min.frac = 0.2
  )
dpred <-
  data.frame(dque, p) %>%
  dplyr::mutate(perc_error = signif(abs(prediction - Price) / Price * 100, 3))
ex1 <- dpred
print(head(ex1))
```

### Example 2: use horsepower, number of passengers, and MPG (in the city) to predict the price
```{r}
dref <- ref %>% dplyr::select(Horsepower, Passengers, MPG.city, Price)
dque <- que %>% dplyr::select(Horsepower, Passengers, MPG.city, Price)
m <- ncol(dref)
p <-
  fvn(
    x = dref[, -m],
    y = dref[, m],
    q = dque[, -m],
    v = 0.2,
    min.pts = 1,
    min.frac = 0.2
  )
dpred <-
  data.frame(dque, p) %>%
  dplyr::mutate(perc_error = signif(abs(prediction - Price) / Price * 100, 3))
ex2 <- dpred
print(head(ex2))
```

### Example 3: user horsepower, number of passengers, MPG (in the city), and wheelbase to predict the price
```{r}
dref <- ref %>% dplyr::select(Horsepower, Passengers, MPG.city, Wheelbase, Price)
dque <- que %>% dplyr::select(Horsepower, Passengers, MPG.city, Wheelbase, Price)
m <- ncol(dref)
p <-
  fvn(
    x = dref[, -m],
    y = dref[, m],
    q = dque[, -m],
    v = 0.2,
    min.pts = 1,
    min.frac = 0.2
  )
dpred <-
  data.frame(dque, p) %>%
  dplyr::mutate(perc_error = signif(abs(prediction - Price) / Price * 100, 3))
ex3 <- dpred
print(head(ex3))
```

### Put 3 examples side by side
```{r}
compare <-
  data.frame(
    que$Price,
    pred1 = ex1$prediction,
    comm1 = ex1$comment,
    nn1 = ex1$n_neighbors,
    err1 = ex1$perc_error,
    pred2 = ex2$prediction,
    comm2 = ex2$comment,
    nn2 = ex2$n_neighbors,
    err2 = ex2$perc_error,
    pred3 = ex3$prediction,
    comm3 = ex3$comment,
    nn3 = ex3$n_neighbors,
    err3 = ex3$perc_error
  )
print(head(compare))
```

This file is to demonstrate how to use FVN, but not to analyze the Cars93 data. Next, I will demonstrate using FVN on classification (coming soon).
